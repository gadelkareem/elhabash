package elhabash

import (
    "bufio"
    "bytes"
    "crypto/tls"
    "errors"
    "fmt"
    "io"
    "net"
    "net/http"
    "net/http/cookiejar"
    "net/http/httputil"
    "net/url"
    "os"
    "os/signal"
    "regexp"
    "runtime/debug"
    "strings"
    "sync"
    "syscall"
    "time"

    "github.com/gadelkareem/cachita"
    "github.com/gadelkareem/faloota"
    h "github.com/gadelkareem/go-helpers"
    "github.com/gadelkareem/quiver"
    "golang.org/x/text/encoding/htmlindex"

    "github.com/PuerkitoBio/goquery"
    "github.com/astaxie/beego/logs"
    "github.com/temoto/robotstxt"
)

var (
    // ErrEmptyHost is returned if a command to be enqueued has an Url with an empty host.
    ErrEmptyHost = errors.New("invalid empty host")

    // ErrDisallowed is returned when the requested Url is disallowed by the robots.txt
    // policy.
    ErrDisallowed = errors.New("disallowed by robots.txt")

    // ErrQueueClosed is returned when a Send call is made on a closed Queue.
    ErrQueueClosed = errors.New("send on a closed queue")
)

// Parse the robots.txt relative path a single time at startup, this can't
// return an error.
var robotsTxtParsedPath, _ = url.Parse("/robots.txt")

const (
    // DefaultCrawlDelay represents the delay to use if there is no robots.txt
    // specified delay.
    DefaultCrawlDelay = 5 * time.Second

    // DefaultUserAgent is the default user agent string.
    GoogleBotUserAgent = "Googlebot/2.1 (+http://www.google.com/bot.html)"

    // DefaultWorkerIdleTTL is the default time-to-live of an idle host worker goroutine.
    // If no Url is sent for a given host within this duration, this host's goroutine
    // is disposed of.
    DefaultWorkerIdleTTL = 5 * time.Second

    MaxAllowedGoRoutines = 10000

    // Max number of commands for client before creating a new one - with a new ProxyFactory
    MaximumClientCommands = 300

    // Max number of Url errors
    MaxUrlErrors    = 20
    MaxMirrorErrors = 1000

    ClientTimeout = 30
)

// A HandlerFunc is a function signature that implements the Handler interface. A function
// with this signature can thus be used as a Handler.
type HandlerFunc func(*goquery.Document, Command, *http.Response, error)
type ErrorHandlerFunc func(*http.Response, error, Command) (bool, bool, bool)

// A Fetcher defines the parameters for running a web crawler.
type Fetcher struct {
    // The Handler to be called for each request. All successfully enqueued requests
    // produce a Handler call.
    Handler      HandlerFunc
    ErrorHandler ErrorHandlerFunc

    // encoding string as in http://www.w3.org/TR/encoding
    DecodeCharset string
    // DisablePoliteness disables fetching and using the robots.txt policies of
    // channels.
    DisablePoliteness bool

    // Default delay to use between requests to a same host if there is no robots.txt
    // crawl delay or if DisablePoliteness is true.
    CrawlDelay time.Duration

    // The time a host-dedicated worker goroutine can stay idle, with no Command to enqueue,
    // before it is stopped and cleared from memory.
    WorkerIdleTTL time.Duration

    MaxWorkers       int
    workersWaitGroup sync.WaitGroup

    // queue holds the Queue to send data to the fetcher and optionally close (stop) it.
    queue        *Queue
    ShuttingDown bool

    // used mainly to display emoiji instead of debug info
    LogLevel int

    // channels maps the host names to its dedicated requests channel, and channelsMu protects
    // concurrent access to the channels field.
    channels []chan Command

    ProxyFactory quiver.ProxyFactory

    Faloota       *faloota.Faloota
    FalootaVerify faloota.Action

    Headers          map[string]string
    cookiesMu        sync.RWMutex
    cookies          []*http.Cookie
    DisableKeepAlive bool

    mirrorsMu            sync.Mutex
    Mirrors              []string
    DisableMirrorTesting bool
    MainHost             string
    Name                 string

    MaxCommandsPerClient int

    // Protect access to UrlErrors
    urlErrorsMu sync.Mutex
    // errors table
    urlErrors map[string]map[string]int

    sleepingMu        sync.Mutex
    sleepingProcesses int

    // exclude filter is higher than the include filter
    ExcludeRegexFilter *regexp.Regexp
    IncludeRegexFilter *regexp.Regexp
    StopString         string

    KeepCrawling bool

    Cache          cachita.Cache
    CacheQueueSize int
    cacheQueueMu   sync.Mutex
    cacheQueue     []*Page

    startTime time.Time

    robotAgentsMu sync.Mutex
    robotAgents   map[string]*robotstxt.Group

    DisableSnapshot bool
    snapshot        *Snapshot
}

// Start starts the Fetcher, and returns the Queue to use to send Commands to be fetched.
func (f *Fetcher) Start(rawUrl string) *Queue {

    rLimit, _ := h.LiftRLimits()
    if rLimit.Cur < 2000 {
        panic("you need to increase file descriptors...")
    }
    logs.Alert("rlimit final %v", rLimit)

    if f.MainHost == "" {
        panic("no MainHost specified...")
    }

    if f.Faloota != nil && f.FalootaVerify == nil {
        panic("no verify func found for faloota")
    }

    f.startTime = time.Now()

    f.robotAgents = make(map[string]*robotstxt.Group)
    f.urlErrors = make(map[string]map[string]int)
    f.urlErrors["urls"] = make(map[string]int)
    f.urlErrors["mirrors"] = make(map[string]int)

    f.queue = &Queue{
        commandsChannel: make(chan Command, 1),
        closed:          make(chan struct{}),
        cancelled:       make(chan struct{}),
        done:            make(chan struct{}),
    }

    if f.CrawlDelay <= 0 {
        f.CrawlDelay = DefaultCrawlDelay
    }
    if f.WorkerIdleTTL <= 0 {
        f.WorkerIdleTTL = DefaultWorkerIdleTTL
    }
    if f.MaxCommandsPerClient <= 0 {
        f.MaxCommandsPerClient = MaximumClientCommands
    }

    if f.ProxyFactory != nil {
        if f.MaxWorkers <= 0 {
            proxiesCount := f.ProxyFactory.TotalCount()
            if proxiesCount > 0 && proxiesCount < MaxAllowedGoRoutines {
                f.MaxWorkers = proxiesCount
            } else if proxiesCount > MaxAllowedGoRoutines {
                f.MaxWorkers = MaxAllowedGoRoutines
            } else {
                f.MaxWorkers = 1
            }
        }
    }

    if f.ErrorHandler == nil {
        f.ErrorHandler = f.HandleError
    }

    logs.Alert("Using %d goroutines.", f.MaxWorkers)

    f.snapshot = NewSnapshot(f.queue, f.Name, f.DisableSnapshot)

    f.testMirrors()

    // Start the one and only queue processing goroutine.
    f.queue.wg.Add(1)
    go f.processQueue()

    parsedUrl, err := url.Parse(rawUrl)
    if err != nil {
        go f.exception(err.Error())
        return nil
    }

    f.ensureGracefulShutdown()

    // Enqueue the seed, which is the first entry in the duplicates map
    return f.Get(parsedUrl)
}

func (f *Fetcher) ensureGracefulShutdown() {
    var gracefulStop = make(chan os.Signal)
    signal.Notify(gracefulStop, syscall.SIGTERM, syscall.SIGINT, os.Interrupt)
    go func() {
        sig := <-gracefulStop
        logs.Emergency("Caught sig: %+v, shutting down gracefully..", sig)
        logs.Emergency("Break again to force exit..")
        signal.Reset()
        f.Shutdown()
    }()
}

func (f *Fetcher) exception(e string) {
    select {
    case <-f.queue.cancelled:
        // already cancelled, no-op
        return
    default:
    }

    _, err := fmt.Fprintf(os.Stderr, "EXCEPTION: %s", e)
    h.LogOnError(err)

    f.Shutdown()
    debug.PrintStack()
    panic(e)
}

func (f *Fetcher) Shutdown() {
    if f.ShuttingDown {
        return
    }
    f.ShuttingDown = true
    logs.Emergency("Shutting down while queue has %d commands...", f.snapshot.queueLength())
    f.cacheQueueMu.Lock()
    if f.Cache != nil && len(f.cacheQueue) > 0 {
        logs.Alert("Flushing cached URLs %d", len(f.cacheQueue))
        f.cachePutMulti()
    }
    f.cacheQueueMu.Unlock()

    h.LogOnError(f.queue.Cancel())
}

func (f *Fetcher) cachePutMulti() {
    for _, page := range f.cacheQueue {
        err := f.Cache.Put(page.Id, page, 0)
        if err != nil {
            logs.Error("ðŸ”¥ Error setting cache: %v", err)
        }
    }
}

// processQueue runs the queue in its own goroutine.
func (f *Fetcher) processQueue() {
    var i int

    logs.Info("Launching %d workers", f.MaxWorkers)
    for i = 0; i < f.MaxWorkers; i++ {
        // Create the infinite queue: the in channel to send on, and the out channel
        // to read from in the host's goroutine, and add to the channels map
        inChanCommands, outChanCommands := make(chan Command, 1), make(chan Command, 1)

        f.channels = append(f.channels, inChanCommands)

        // Start the infinite queue goroutine for this host
        go sliceIQ(inChanCommands, outChanCommands)
        // Start the working goroutine for this host
        f.workersWaitGroup.Add(1)
        go f.processChan(outChanCommands, i)
    }

    i = 0
loop:
    for command := range f.queue.commandsChannel {

        if command == nil {
            // Special case, when the Queue is closed, a nil command is sent, use this
            // indicator to check for the closed signal, instead of looking on every loop.
            select {
            case <-f.queue.closed:
                logs.Info("Got close signal on main queue")
                // Close signal, exit loop
                break loop
            default:
                // Keep going
            }
        }
        select {
        case <-f.queue.cancelled:
            logs.Info("Got cancel signal on main queue")
            // queue got stop, drain
            continue
        default:
            // go on
        }

        f.snapshot.addCommandInQueue(f.uniqueId(command.Url(), command.Method()), command)

        // Send the request
        // logs.Debug("New command for Url: %s", command.Url())
        f.channels[i] <- command
        i++
        if i == len(f.channels) {
            i = 0
        }

    }

    for _, ch := range f.channels {
        close(ch)
        for range ch {
        }
    }

    f.workersWaitGroup.Wait()
    f.queue.wg.Done()
    logs.Alert("All commands completed in %s", time.Since(f.startTime))
}

// Goroutine for a host's worker, processing requests for all its URLs.
func (f *Fetcher) processChan(outChanCommands <-chan Command, routineIndex int) {
    var (
        agent *robotstxt.Group
        ttl   <-chan time.Time
        // add some random seconds for each channel
        delay              = f.CrawlDelay
        httpClient         = f.NewClient(true, true)
        httpClientCommands = 0
        restartClient      = false
    )

    // tata tata
    logs.Info("Channel %d is starting..", routineIndex)
    if routineIndex < 10 {
        time.Sleep(time.Duration(routineIndex) + 2*time.Second)
    }

loop:
    for {
        select {
        case <-f.queue.cancelled:
            break loop
        case command, ok := <-outChanCommands:
            if !ok {
                logs.Info("Channel %d was closed, terminating..", routineIndex)
                // Terminate this goroutine, channel is closed
                break loop
            }

            // was it stop during the wait? check again
            select {
            case <-f.queue.cancelled:
                logs.Info("Channel %d was cancelled, terminating..", routineIndex)
                break loop
            default:
                // go on
            }

            restartClient = false

            logs.Debug("Channel %d received new command %s", routineIndex, command.Url())

            if !f.DisablePoliteness {
                agent = f.getRobotAgent(command.Url())
                // Initialize the crawl delay
                if agent != nil && agent.CrawlDelay > 0 {
                    delay = agent.CrawlDelay
                }
            }

            if command.HttpClient() == nil {
                command.SetHttpClient(httpClient)
            }

            if f.DisablePoliteness || agent == nil || agent.Test(command.Url().Path) {

                // path allowed, process the request
                res, err, isCached := f.Request(command, delay)

                if !isCached {
                    httpClientCommands++

                    var statusCode int
                    if res != nil {
                        statusCode = res.StatusCode
                    }
                    logs.Info("[%d][%d] %s %s", routineIndex, statusCode, command.Method(), command.Url())
                }

                restartClient = f.visit(command, res, err, isCached)

            } else {
                // path disallowed by robots.txt
                f.visit(command, nil, ErrDisallowed, false)
            }

            f.snapshot.removeCommandInQueue(f.uniqueId(command.Url(), command.Method()))

            // Every time a command is received, reset the ttl channel
            ttl = time.After(f.WorkerIdleTTL)

            if restartClient || httpClientCommands > f.MaxCommandsPerClient {
                if f.LogLevel < logs.LevelNotice {
                    fmt.Print("ðŸ‘…")
                }
                logs.Info("Channel %d needs restart after %d commands..", routineIndex, httpClientCommands)
                go f.processChan(outChanCommands, routineIndex)
                return
            }

        case <-ttl:
            if f.snapshot.queueLength() != 0 {
                logs.Debug("Channel %d was trying to timeout while queue length is %d", routineIndex, f.snapshot.queueLength())
                ttl = time.After(f.WorkerIdleTTL)
                continue
            }
            logs.Alert("Channel %d with %d unique urls", routineIndex, f.snapshot.uniqueUrlsLength())
            go f.Shutdown()
            break loop
        }
    }
    for range outChanCommands {
    }

    f.workersWaitGroup.Done()
}

// Prepare and execute the request for this Command.
func (f *Fetcher) Request(cmd Command, delay time.Duration) (response *http.Response, err error, isCached bool) {
    var (
        cacheId string
    )

    isCached = false

    httpClient := cmd.HttpClient()
    if httpClient == nil {
        go f.exception("No HTTP Client in the command:" + cmd.MirrorUrl().String())
        return
    }

    req := cmd.Request()
    if req == nil {
        req, err = http.NewRequest(cmd.Method(), cmd.MirrorUrl().String(), nil)
        if err != nil {
            return
        }
    }
    cmd.SetRequest(req)
    rawUrl := cmd.MirrorUrl().String()

    forceUpdateCache := false
    if cmd.Method() == http.MethodGet {
        cacheId = f.cacheId(cmd.Url(), cmd.Method())
        if cacheId != "" {
            response = f.getCachedResponse(cacheId)
            if response != nil {
                isCached = true
                response.Request = req
                if f.LogLevel < logs.LevelNotice {
                    fmt.Print("âž°")
                }
                logs.Info("Got cache for Url: %s cacheId: %s", rawUrl, cacheId)
                return
            }
            forceUpdateCache = true
        }
        logs.Info("No cache found for Url: %s cacheId: %s", rawUrl, cacheId)
    }

    if f.DisableKeepAlive {
        req.Close = true
    }

    if cmd.ReferrerUrl() != nil {
        referrer := cmd.ReferrerUrl()
        referrer.Scheme = cmd.Url().Scheme
        logs.Debug("Setting referrer %s for %s", referrer, cmd.Url())
        req.Header.Set("Referer", referrer.String())
    }

    f.passCookies(httpClient, cmd)

    f.sleeping(false)
    logs.Info("Taking a nap for %s", delay)
    time.Sleep(delay)
    f.sleeping(true)

    for k, v := range f.Headers {
        req.Header.Set(k, v)
    }

    if req.Header.Get("User-Agent") == "" {
        req.Header.Set("User-Agent", cmd.HttpClient().UserAgent)
    }

    if cmd.Url().Scheme == "http" && req.Header.Get("Request-IP") == "" && cmd.HttpClient().ProxyOutboundIp != "" {
        req.Header.Set("Request-IP", cmd.HttpClient().ProxyOutboundIp)
        logs.Debug("Using proxy:%s", cmd.HttpClient().ProxyOutboundIp)
    }

    // Do the request.
    response, err = cmd.HttpClient().Do(req)
    if err != nil {
        return
    }

    if response != nil {
        if response.StatusCode == http.StatusOK ||
            (httpClient.Mirror == "" && // if we ll cache a redirect or not found then make sure it's not a mirror
                (response.StatusCode == http.StatusMovedPermanently || response.StatusCode == http.StatusNotFound)) {
            if cacheId != "" {
                response.Request = req
                f.cacheResponse(cmd, cacheId, response, forceUpdateCache)
                logs.Info("Cached Url: %s cacheId: %s statusCode:%d", rawUrl, cacheId, response.StatusCode)
            }
            if f.LogLevel < logs.LevelNotice {
                fmt.Print("ðŸš€")
            }
        }
    }

    return
}

// Call the Handler for this Command. Closes the response's body.
func (f *Fetcher) visit(cmd Command, response *http.Response, err error, isCached bool) (restartClient bool) {
    restartClient = false

    defer func() {
        if response != nil && response.Body != nil {
            response.Body.Close()
        }
    }()
    hasErrors, brokenMirror, brokenUrl := f.ErrorHandler(response, err, cmd)
    if hasErrors {
        logs.Debug("ðŸ”¥ Error with Url: %s proxy: %s Error: %v", cmd.MirrorUrl(), cmd.HttpClient().ProxyURL, err)
        if brokenMirror {
            restartClient = true
        }
        if brokenUrl { // too many failures for this Url
            if err != nil && err != ErrDisallowed {
                logs.Error("ðŸ”¥ Url %s keeps failing, Error: %v", cmd.Url(), err)
            }
            return
        } else if f.LogLevel < logs.LevelNotice {
            fmt.Print("ðŸ”¥")
        }
        if !isCached {
            cmd.SetHttpClient(nil) // make it light for the queue
            f.queue.Send(cmd)
        }
        return
    }

    var document *goquery.Document
    if strings.Contains(response.Header.Get("content-type"), "text") {
        logs.Info("Parsing Url %s", cmd.Url())
        var body io.Reader
        if f.DecodeCharset != "" && f.DecodeCharset != "utf-8" {
            e, err := htmlindex.Get(f.DecodeCharset)
            if err != nil {
                go f.exception("Could not find decoding charset " + f.DecodeCharset + " " + err.Error() + " " + cmd.Url().String())
                return
            }
            body = e.NewDecoder().Reader(response.Body)
        } else {
            body = response.Body
        }
        document, err = goquery.NewDocumentFromReader(body)
        if err != nil {
            logs.Error("Error parsing html %s %s - %s", cmd.Method(), cmd.Url(), err)
            return
        }
        if f.KeepCrawling {
            f.enqueueLinks(cmd.MirrorUrl(), document)
        }
    }

    logs.Info("Passing to handler Url %s", cmd.Url())
    if f.Handler != nil {
        f.Handler(document, cmd, response, err)
    }

    return

}

func (f *Fetcher) NewClient(withProxy, withMirror bool) *Client {
    var err error

    httpClient := &Client{}

    httpClient.Jar, err = cookiejar.New(nil)
    if err != nil {
        logs.Error("Error setting cookie jar : %s", err)
    }

    if withProxy && f.ProxyFactory != nil {
        httpClient.ProxyOutboundIp, httpClient.ProxyURL = f.ProxyFactory.RandomProxy()
        httpClient.UpdateDirectProxyURL()
        if httpClient.ProxyOutboundIp == "" && httpClient.ProxyURL == nil {
            go f.exception("Running out of proxies..")
            return nil
        }
    }

    keepAlive := ClientTimeout * 4 * time.Second
    idleConnTimeout := ClientTimeout * 4 * time.Second
    maxIdleConns := 100
    if f.DisableKeepAlive {
        keepAlive = 0
        idleConnTimeout = 0
        maxIdleConns = 1
    }
    transport := &http.Transport{
        TLSClientConfig: &tls.Config{InsecureSkipVerify: true},
        Dial: (&net.Dialer{
            Timeout:       ClientTimeout * time.Second,
            KeepAlive:     keepAlive,
            FallbackDelay: -1,
        }).Dial,
        TLSHandshakeTimeout:   ClientTimeout * time.Second,
        ResponseHeaderTimeout: ClientTimeout * time.Second,
        IdleConnTimeout:       idleConnTimeout,
        ExpectContinueTimeout: 1 * time.Second,
        DisableKeepAlives:     f.DisableKeepAlive,
        DisableCompression:    false,
        MaxIdleConns:          maxIdleConns,
        MaxIdleConnsPerHost:   maxIdleConns,
    }
    httpClient.Timeout = ClientTimeout * 2 * time.Second

    if httpClient.ProxyURL != nil {
        transport.Proxy = http.ProxyURL(httpClient.ProxyURL)
        if httpClient.ProxyOutboundIp != "" {
            transport.ProxyConnectHeader = http.Header{"Request-IP": []string{httpClient.ProxyOutboundIp}}
        }
    }
    httpClient.Transport = transport
    httpClient.UserAgent = userAgents[h.RandomNumber(0, len(userAgents))]
    httpClient.CheckRedirect = func(req *http.Request, via []*http.Request) error {
        return http.ErrUseLastResponse
    }

    host := f.MainHost
    if withMirror && len(f.Mirrors) > 0 {
        f.mirrorsMu.Lock()
        defer f.mirrorsMu.Unlock()
        httpClient.Mirror = f.Mirrors[h.RandomNumber(0, len(f.Mirrors))]
        host = httpClient.Mirror
    }

    f.bypass(host, httpClient)

    logs.Info("New client created with Mirror: %s, proxy: %s", httpClient.Mirror, httpClient.ProxyURL)

    return httpClient
}

func (f *Fetcher) AddCookies(cookies []*http.Cookie) {
    f.cookiesMu.Lock()
    defer f.cookiesMu.Unlock()
    if f.cookies == nil {
        f.cookies = cookies
    }

    added := false
    for _, c := range cookies {
        for x, ck := range f.cookies {
            if c.Name == ck.Name {
                f.cookies[x] = c
                added = true
            }
        }
        if !added {
            f.cookies = append(f.cookies, c)
            added = false
        }
    }
}

func (f *Fetcher) HandleError(response *http.Response, err error, cmd Command) (hasErrors, brokenMirror, brokenUrl bool) {

    hasErrors, brokenMirror, brokenUrl = false, false, false

    if err == ErrDisallowed {
        return true, false, true
    }

    httpClient := cmd.HttpClient()
    if response == nil {
        hasErrors = true
    } else if response.StatusCode != http.StatusOK {
        hasErrors = true
        if response.StatusCode == http.StatusServiceUnavailable &&
            strings.Contains(response.Header.Get("Server"), "cloudflare") {
            logs.Error("ðŸ”¥ðŸ”¥ Got Cloudflare error for URL: %s Proxy: %s", cmd.MirrorUrl(), httpClient.ProxyURL)

            brokenMirror = !f.bypass(cmd.MirrorUrl().Host, httpClient)
            goto recordErr
        }
        if response.StatusCode == 549 { // Zaki Edra is still assigning the IP so resend cmd
            goto recordErr
        }
        if f.ProxyFactory != nil &&
            httpClient.ProxyURL != nil &&
            (response.StatusCode == 550 || // invalid IP
                response.StatusCode == 551 || // banned IP
                response.StatusCode == 552) { // auth problem
            go f.exception(fmt.Sprintf("ðŸ”¥ðŸ”¥ proxy %s has Auth problem! status: %d IP: %s URL: %s\n", httpClient.ProxyURL, response.StatusCode, httpClient.ProxyOutboundIp, cmd.MirrorUrl()))
            return
        } else if response.StatusCode == http.StatusMovedPermanently ||
            response.StatusCode == http.StatusFound {
            redirectUrl, err := response.Location()
            if err != nil {
                go f.exception(fmt.Sprintf("Error getting redirect Url %s for Url %s Error: %v", redirectUrl, cmd.MirrorUrl(), err))
                return
            }
            logs.Notice("ðŸ”¥ %d %s redirects to : %s", response.StatusCode, cmd.MirrorUrl(), redirectUrl)
            if redirectUrl.Host == cmd.MirrorUrl().Host {
                redirectUrl.Host = f.MainHost
            }
            if !f.excludeUrl(redirectUrl) {
                f.Get(redirectUrl)
            }
            // mark Url as broken to skip re-queueing & continue to add mirror error
            brokenUrl = true
            return

        } else if response.StatusCode == http.StatusNotFound {
            logs.Debug("ðŸ”¥ Error with Url: %s Error: 404 Not Found", cmd.MirrorUrl())
            brokenUrl = true
            return
        } else {
            logs.Notice("ðŸ”¥ %d %s", response.StatusCode, cmd.MirrorUrl())
        }
    }

    if err != nil {
        hasErrors = true
        if netError, ok := err.(net.Error); ok && netError.Timeout() {
            if f.ProxyFactory != nil && strings.Contains(err.Error(), "proxyconnect") {
                logs.Error("ðŸ”¥ðŸ”¥ proxy %s failed! Error: %v", httpClient.ProxyURL.String(), err)
            } else if strings.Contains(err.Error(), "Connection refused") {
                logs.Error("ðŸ”¥ðŸ”¥ proxy Connection refused %v", err)
            } else {
                logs.Error("ðŸ”¥ TIMEOUT %s proxy: %s", err, httpClient.ProxyURL.String())
            }
        }
    }

    if !hasErrors {
        return
    }

recordErr:
    f.urlErrorsMu.Lock()
    defer f.urlErrorsMu.Unlock()
    rawUrl := cmd.Url().String()
    if _, exists := f.urlErrors["urls"][rawUrl]; exists {
        f.urlErrors["urls"][rawUrl]++
    } else {
        f.urlErrors["urls"][rawUrl] = 1
    }

    if !brokenUrl {
        brokenUrl = f.urlErrors["urls"][rawUrl] > MaxUrlErrors
    }

    if f.Mirrors != nil && httpClient != nil {
        mirror := httpClient.Mirror
        if _, exists := f.urlErrors["mirrors"][mirror]; exists {
            f.urlErrors["mirrors"][mirror]++
            if f.urlErrors["mirrors"][mirror] > MaxMirrorErrors {
                f.mirrorsMu.Lock()
                defer f.mirrorsMu.Unlock()
                if len(f.Mirrors) < 2 {
                    f.Mirrors = nil
                    logs.Error("ðŸ”¥ðŸ”¥ mirrors.. %s keeps failing.. Disabling it for now..", mirror)
                } else {
                    for i, m := range f.Mirrors {
                        if m == mirror {
                            f.Mirrors = append(f.Mirrors[:i], f.Mirrors[i+1:]...)
                            logs.Error("ðŸ”¥ðŸ”¥ mirror: %s keeps failing.. removing it", mirror)
                            break
                        }
                    }
                }
                brokenMirror = true
            }
        } else {
            f.urlErrors["mirrors"][mirror] = 1
        }
    }

    return
}

func (f *Fetcher) passCookies(client *Client, cmd Command) {
    f.cookiesMu.RLock()
    defer f.cookiesMu.RUnlock()
    if f.cookies == nil {
        return
    }
    client.Jar.SetCookies(cmd.MirrorUrl(), f.cookies)
    logs.Debug("Set Cookies: %+v", f.cookies)
}

// Start starts the Fetcher, and returns the Queue to use to send Commands to be fetched.
func (f *Fetcher) Get(u *url.URL) *Queue {
    _, err := f.queue.SendStringGet(u)
    if err != nil {
        if err != ErrQueueClosed {
            logs.Warn("enqueue get %s - %s", u, err)
        }
    } else {
        f.snapshot.addUniqueUrl(f.uniqueId(u, "GET"))
        logs.Debug("New URL %s added", u)
    }

    return f.queue
}

func (f *Fetcher) excludeUrl(u *url.URL) bool {

    if f.StopString != "" && strings.Contains(u.String(), f.StopString) {
        go f.exception("Got stop string in the URL: " + u.String())
        return true
    }

    if u.Host != f.MainHost {
        return true
    }

    if f.snapshot.uniqueUrlExists(f.uniqueId(u, "GET")) {
        return true
    }

    requestUri := u.RequestURI()
    if f.ExcludeRegexFilter != nil && f.ExcludeRegexFilter.MatchString(requestUri) {
        return true
    }

    if f.IncludeRegexFilter != nil {
        return !f.IncludeRegexFilter.MatchString(requestUri)
    }

    return false
}

// Get the robots.txt User-Agent-specific group.
func (f *Fetcher) getRobotAgent(u *url.URL) *robotstxt.Group {
    f.robotAgentsMu.Lock()
    defer f.robotAgentsMu.Unlock()

    if agent, exists := f.robotAgents[u.Host]; exists {
        return agent
    }

    // Must send the robots.txt request.
    robotsTxtUrl := u.ResolveReference(robotsTxtParsedPath)
    // Enqueue the robots.txt request first.

    var res *http.Response
    err := h.Retry(func() (e error) {
        cmd := &Cmd{U: robotsTxtUrl, M: "GET", C: f.NewClient(true, false)}
        res, e, _ = f.Request(cmd, 0)
        if e != nil {
            logs.Error("Error fetching robots.txt: %v", e)
        }
        return e
    }, 15)

    if res == nil || err != nil {
        errString := "Error fetching robots.txt: "
        if err != nil {
            errString += err.Error()
        }
        go f.exception(errString)
        return nil
    }
    if res.Body != nil {
        defer res.Body.Close()
    }
    robData, err := robotstxt.FromResponse(res)
    if err != nil {
        go f.exception("Error parsing robots.txt: " + err.Error())
        return nil
    }
    f.robotAgents[u.Host] = robData.FindGroup(GoogleBotUserAgent)

    return f.robotAgents[u.Host]
}

func (f *Fetcher) sleeping(wakeup bool) {
    if f.LogLevel != logs.LevelDebug {
        return
    }
    f.sleepingMu.Lock()
    defer f.sleepingMu.Unlock()
    if wakeup {
        f.sleepingProcesses--
    } else {
        f.sleepingProcesses++
    }
    logs.Info("%d Goroutines sleeping", f.sleepingProcesses)
}

func (f *Fetcher) getCachedResponse(cacheId string) *http.Response {
    var page *Page
    err := f.Cache.Get(cacheId, &page)
    if err != nil && !cachita.IsErrorOk(err) {
        logs.Error("ðŸ”¥ Error getting cache: %v", err)
    }
    if page == nil {
        return nil
    }

    reader := bufio.NewReader(bytes.NewReader(page.Response))
    response, err := http.ReadResponse(reader, nil)
    if err != nil {
        logs.Error("Error reading cached response: %s id: %s URL: %s response size: %d", err, cacheId, page.RawUrl, len(page.Response))
    }

    return response
}

func (f *Fetcher) cacheResponse(cmd Command, cacheId string, response *http.Response, forceUpdate bool) {

    if cmd.isDisableCache() {
        return
    }

    responseDump, err := httputil.DumpResponse(response, true)
    if err != nil {
        logs.Error("Error creating response dump: %s ", err)
        return
    }

    page := &Page{
        Id:       cacheId,
        Response: responseDump,
        RawUrl:   cmd.Url().String(),
    }

    logs.Info("new cache created for Url: %s", cmd.Url())
    go func(p *Page) {
        if f.CacheQueueSize < 2 {
            logs.Info("Caching Url %s", cmd.Url())
            err = f.Cache.Put(p.Id, p, 0)
            if err != nil {
                logs.Error("ðŸ”¥ Error putting cache: %v", err)
            }
            return
        }

        f.cacheQueueMu.Lock()
        defer f.cacheQueueMu.Unlock()
        for _, v := range f.cacheQueue {
            if page.Id == v.Id {
                return
            }
        }
        f.cacheQueue = append(f.cacheQueue, p)
        if len(f.cacheQueue) > f.CacheQueueSize {
            logs.Info("Caching multiple URLs %d", len(f.cacheQueue))
            f.cachePutMulti()
            f.cacheQueue = nil
        }
    }(page)
}

func (f *Fetcher) InvalidateCache(u *url.URL, method string) {
    cacheId := f.cacheId(u, method)
    if cacheId != "" {
        f.Cache.Invalidate(cacheId)
    }
}

func (f *Fetcher) uniqueId(u *url.URL, method string) string {
    var rawUrl string
    if u.Host != f.MainHost {
        rawUrl = u.String()
    } else {
        rawUrl = u.Scheme + u.RequestURI()
    }
    return method + rawUrl
}

func (f *Fetcher) cacheId(u *url.URL, method string) string {
    if f.Cache == nil {
        return ""
    }
    rawUrl := u.RequestURI()
    if u.Host != f.MainHost {
        rawUrl = u.String()
    }
    return cachita.Id(rawUrl, method)
}

func (f *Fetcher) enqueueLinks(baseUrl *url.URL, document *goquery.Document) {
    total := 0
    // Enqueue all links as GET requests
    document.Find("a[href]").Each(func(i int, selection *goquery.Selection) {
        val, _ := selection.Attr("href")
        // Resolve address
        u, err := baseUrl.Parse(val)
        if err != nil {
            // fmt.Printf("error: resolve Url %selection - %selection\n", val, err)
            return
        }
        if u.Host == baseUrl.Host {
            u.Host = f.MainHost
        }

        if !f.excludeUrl(u) {
            u.Fragment = ""
            f.Get(u)
            total++
        }
    })
    logs.Info("Found %d total links on %s", total, baseUrl)
}

func (f *Fetcher) testMirrors() {
    if f.Mirrors == nil || f.DisableMirrorTesting {
        return
    }
    f.mirrorsMu.Lock()
    defer f.mirrorsMu.Unlock()
    httpClient := f.NewClient(true, false)
    httpClient.CheckRedirect = nil
    var mirrors []string
    for _, mirror := range f.Mirrors {
        rawUrl := "http://" + mirror + "/"
        u, err := url.Parse(rawUrl)
        if err != nil {
            go f.exception(err.Error())
            return
        }
        f.InvalidateCache(u, "GET")
        f.bypass(mirror, httpClient)
        // Do the request.
        rs, err, _ := f.Request(&Cmd{U: u, M: "GET", C: httpClient}, 0)
        if err != nil || rs.StatusCode != http.StatusOK {
            errorString := mirror + " is broken "
            if err != nil {
                errorString += " Error:" + err.Error()
            }
            if rs != nil {
                errorString += fmt.Sprintf(" Status: %d", rs.StatusCode)
                // fmt.Printf("%+v", rs)
            }
            logs.Error(errorString)
        } else {
            logs.Alert("%s looks good", mirror)
            mirrors = append(mirrors, mirror)
        }
        if rs != nil {
            rs.Body.Close()
        }
    }
    f.Mirrors = mirrors

}

func (f *Fetcher) bypass(host string, httpClient *Client) bool {
    if f.Faloota == nil {
        return true
    }
    u := "http://" + host
    r, err, _ := f.Request(&Cmd{U: h.ParseUrl(u), M: "GET", C: httpClient, DisableMirror: true}, 0)
    if err != nil || r == nil || r.Body == nil || r.StatusCode != http.StatusOK {
        logs.Error("Request error on url %s proxy: %s Error: %v - will try to bypass", u, httpClient.ProxyURL, err)
    } else {
        r.Body.Close()
        return true
    }
    logs.Info("Faloota starting for host %s with proxy %s", host, httpClient.DirectProxyURL)
    cookies, err := f.Faloota.BypassOnce(u, httpClient.DirectProxyURL, httpClient.UserAgent, f.FalootaVerify)
    if err != nil {
        logs.Error("Falouta error on url %s Error: %v", u, err)
        return false
    }
    httpClient.Jar.SetCookies(h.ParseUrl(u), cookies)
    logs.Debug("added cookies to client %+v", cookies)
    return true
}
